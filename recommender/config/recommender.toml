[prep]
source_db = "../../output/anilist.sqlite"
semantic_ids_path = "../data/semantic_ids/conservative/semantic_ids.jsonl"
out_dir = "../output"
rebuild = true
seed = 42
target_examples = 400000
split_task_a = 0.50
split_task_b = 0.25
split_task_c = 0.25
max_task_a_templates_per_anime = 12
task_b_min_history = 4
task_b_max_history = 30
task_b_mask_modes = ["last", "random"]
task_b_positive_score_min = 7
task_b_allowed_statuses = [1, 2, 3, 4, 6]
export_parquet = false
write_manifest = true

[train]
base_model = "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
dry_run_model_name = "sshleifer/tiny-gpt2"
train_jsonl = "../output/llm_prep_train.jsonl"
out_dir = "../output/train"
env_file = "../.env"
rebuild = true
seed = 42
device = "auto"
precision = "bf16"
max_seq_len = 1024
learning_rate = 0.0002
weight_decay = 0.0
warmup_ratio = 0.03
per_device_train_batch_size = 1
per_device_eval_batch_size = 1
gradient_accumulation_steps = 16
gradient_checkpointing = true
max_grad_norm = 1.0
checkpoint_every_steps = 500
save_total_limit = 3
resume_from = ""
hf_token = ""
hf_token_env = "HF_TOKEN"
dry_run = true
dry_run_max_steps = 20
dry_run_sample_rows = 2048
dry_run_out_subdir = "dry_run"

[train.lora]
enabled = true
r = 16
alpha = 32
dropout = 0.05
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

[train.tokens]
semantic_vocab_path = "../data/semantic_ids/conservative/semantic_vocab.json"
add_special_tokens = true

[train.tokens.warm_start]
enabled = false
semantic_ids_path = "../data/semantic_ids/conservative/semantic_ids.jsonl"
rqvae_checkpoint_path = "../../tokenizer/artifacts/rqvae_conservative/rqvae_best_infer.pt"
ridge_lambda = 0.001
max_fit_samples = 50000

[train.phase1]
epochs = 2
weight_task_a = 0.85
weight_task_b = 0.10
weight_task_c = 0.05
max_domain_rows = 0

[train.phase2]
epochs = 3
weight_task_a = 0.10
weight_task_b = 0.45
weight_task_c = 0.45
max_domain_rows = 0

[train.general_mix]
ratio = 0.15
sources = ["SlimOrca", "OpenHermes"]
max_rows_per_source = 60000
cache_dir = "../output/hf_cache"
seed = 42

[train.eval]
english_eval_dataset = "wikitext2"
eval_every_steps = 500
eval_max_samples = 512
eval_batch_size = 2

[train.wandb]
enabled = false
mode = "offline"
project = ""
entity = ""
api_key = ""
run_name = ""
group = ""
tags = []
project_env = "WANDB_PROJECT"
entity_env = "WANDB_ENTITY"
api_key_env = "WANDB_API_KEY"
